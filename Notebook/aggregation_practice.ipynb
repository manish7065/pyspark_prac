{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 10:14:48 WARN Utils: Your hostname, manu-pc resolves to a loopback address: 127.0.1.1; using 192.168.157.41 instead (on interface wlp58s0)\n",
      "24/08/29 10:14:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/29 10:14:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# rading the data from csv file and creating a df\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Agg_trial\").getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|employee_id|FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "|        198|    Donald| OConnell|DOCONNEL|650.507.9833|21-JUN-07|  SH_CLERK|  2600|          NULL|       124|           50|\n",
      "|        199|   Douglas|    Grant|  DGRANT|650.507.9844|13-JAN-08|  SH_CLERK|  2600|          NULL|       124|           50|\n",
      "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|17-SEP-03|   AD_ASST|  4400|          NULL|       101|           10|\n",
      "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|17-FEB-04|    MK_MAN| 13000|          NULL|       100|           20|\n",
      "|        202|       Pat|      Fay|    PFAY|603.123.6666|17-AUG-05|    MK_REP|  6000|          NULL|       201|           20|\n",
      "|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|07-JUN-02|    HR_REP|  6500|          NULL|       101|           40|\n",
      "|        204|   Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|          NULL|       101|           70|\n",
      "|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|          NULL|       101|          110|\n",
      "|        206|   William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|          NULL|       205|          110|\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|          NULL|      NULL|           90|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|          NULL|       100|           90|\n",
      "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|          NULL|       100|           90|\n",
      "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|          NULL|       102|           60|\n",
      "|        104|     Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|          NULL|       103|           60|\n",
      "|        105|     David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|          NULL|       103|           60|\n",
      "|        106|     Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|          NULL|       103|           60|\n",
      "|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|          NULL|       103|           60|\n",
      "|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|          NULL|       101|          100|\n",
      "|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|          NULL|       108|          100|\n",
      "|        110|      John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|          NULL|       108|          100|\n",
      "+-----------+----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/29 10:15:02 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"../data/employees.csv\", inferSchema=True, header=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+------------+----------+------+--------------+----------+-------------+\n",
      "|employee_id|first_name|last_name|   email|PHONE_NUMBER|joining_date|    job_id|salary|commission_pct|manager_id|department_id|\n",
      "+-----------+----------+---------+--------+------------+------------+----------+------+--------------+----------+-------------+\n",
      "|        198|    Donald| OConnell|DOCONNEL|650.507.9833|   21-JUN-07|  SH_CLERK|  2600|          NULL|       124|           50|\n",
      "|        199|   Douglas|    Grant|  DGRANT|650.507.9844|   13-JAN-08|  SH_CLERK|  2600|          NULL|       124|           50|\n",
      "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|   17-SEP-03|   AD_ASST|  4400|          NULL|       101|           10|\n",
      "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|   17-FEB-04|    MK_MAN| 13000|          NULL|       100|           20|\n",
      "|        202|       Pat|      Fay|    PFAY|603.123.6666|   17-AUG-05|    MK_REP|  6000|          NULL|       201|           20|\n",
      "|        203|     Susan|   Mavris| SMAVRIS|515.123.7777|   07-JUN-02|    HR_REP|  6500|          NULL|       101|           40|\n",
      "|        204|   Hermann|     Baer|   HBAER|515.123.8888|   07-JUN-02|    PR_REP| 10000|          NULL|       101|           70|\n",
      "|        205|   Shelley|  Higgins|SHIGGINS|515.123.8080|   07-JUN-02|    AC_MGR| 12008|          NULL|       101|          110|\n",
      "|        206|   William|    Gietz|  WGIETZ|515.123.8181|   07-JUN-02|AC_ACCOUNT|  8300|          NULL|       205|          110|\n",
      "|        100|    Steven|     King|   SKING|515.123.4567|   17-JUN-03|   AD_PRES| 24000|          NULL|      NULL|           90|\n",
      "|        101|     Neena|  Kochhar|NKOCHHAR|515.123.4568|   21-SEP-05|     AD_VP| 17000|          NULL|       100|           90|\n",
      "|        102|       Lex|  De Haan| LDEHAAN|515.123.4569|   13-JAN-01|     AD_VP| 17000|          NULL|       100|           90|\n",
      "|        103| Alexander|   Hunold| AHUNOLD|590.423.4567|   03-JAN-06|   IT_PROG|  9000|          NULL|       102|           60|\n",
      "|        104|     Bruce|    Ernst|  BERNST|590.423.4568|   21-MAY-07|   IT_PROG|  6000|          NULL|       103|           60|\n",
      "|        105|     David|   Austin| DAUSTIN|590.423.4569|   25-JUN-05|   IT_PROG|  4800|          NULL|       103|           60|\n",
      "|        106|     Valli|Pataballa|VPATABAL|590.423.4560|   05-FEB-06|   IT_PROG|  4800|          NULL|       103|           60|\n",
      "|        107|     Diana|  Lorentz|DLORENTZ|590.423.5567|   07-FEB-07|   IT_PROG|  4200|          NULL|       103|           60|\n",
      "|        108|     Nancy|Greenberg|NGREENBE|515.124.4569|   17-AUG-02|    FI_MGR| 12008|          NULL|       101|          100|\n",
      "|        109|    Daniel|   Faviet| DFAVIET|515.124.4169|   16-AUG-02|FI_ACCOUNT|  9000|          NULL|       108|          100|\n",
      "|        110|      John|     Chen|   JCHEN|515.124.4269|   28-SEP-05|FI_ACCOUNT|  8200|          NULL|       108|          100|\n",
      "+-----------+----------+---------+--------+------------+------------+----------+------+--------------+----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rename_column = {\n",
    "        'DEPARTMENT_ID':'department_id', \n",
    "        'FIRST_NAME':'first_name', \n",
    "        'LAST_NAME':'last_name', \n",
    "        'EMAIL':'email', \n",
    "        'HIRE_DATE':'joining_date', \n",
    "        'JOB_ID':'job_id', \n",
    "        'SALARY':'salary', \n",
    "        'COMMISSION_PCT':'commission_pct', \n",
    "        'MANAGER_ID':'manager_id'\n",
    "        }\n",
    "\n",
    "\n",
    "for old_column, new_column in rename_column.items():\n",
    "    df = df.withColumnRenamed(old_column,new_column)\n",
    "df.show()\n",
    "\n",
    "## otherwise df.withColumnRenamed(\"SALARY\",'salary) can also be used multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salary per department "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|department_id|total_salary|\n",
      "+-------------+------------+\n",
      "|           20|       19000|\n",
      "|           40|        6500|\n",
      "|          100|       51608|\n",
      "|           10|        4400|\n",
      "|           50|       85600|\n",
      "|           70|       10000|\n",
      "|           90|       58000|\n",
      "|           60|       28800|\n",
      "|          110|       20308|\n",
      "|           30|       24900|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "# aggregate arround the department\n",
    "df.groupby('department_id').agg(sum(\"SALARY\").alias(\"total_salary\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|department_id|total_salary|\n",
      "+-------------+------------+\n",
      "|           20|       19000|\n",
      "|           40|        6500|\n",
      "|          100|       51608|\n",
      "|           10|        4400|\n",
      "|           50|       85600|\n",
      "|           70|       10000|\n",
      "|           90|       58000|\n",
      "|           60|       28800|\n",
      "|          110|       20308|\n",
      "|           30|       24900|\n",
      "+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"employees\")\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT department_id, sum(salary) as total_salary from employees\n",
    "GROUP BY department_id\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------------+\n",
      "|department_id|sum(salary)|       avg(salary)|\n",
      "+-------------+-----------+------------------+\n",
      "|           20|      19000|            9500.0|\n",
      "|           40|       6500|            6500.0|\n",
      "|          100|      51608| 8601.333333333334|\n",
      "|           10|       4400|            4400.0|\n",
      "|           50|      85600|3721.7391304347825|\n",
      "|           70|      10000|           10000.0|\n",
      "|           90|      58000|19333.333333333332|\n",
      "|           60|      28800|            5760.0|\n",
      "|          110|      20308|           10154.0|\n",
      "|           30|      24900|            4150.0|\n",
      "+-------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementing multiple aggregatiion\n",
    "\n",
    "from pyspark.sql.functions import avg\n",
    "df.groupBy('department_id').agg(sum('salary'),avg('salary')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+---------------------+\n",
      "|department_id|sum(salary)|round(avg(salary), 2)|\n",
      "+-------------+-----------+---------------------+\n",
      "|           20|      19000|               9500.0|\n",
      "|           40|       6500|               6500.0|\n",
      "|          100|      51608|              8601.33|\n",
      "|           10|       4400|               4400.0|\n",
      "|           50|      85600|              3721.74|\n",
      "|           70|      10000|              10000.0|\n",
      "|           90|      58000|             19333.33|\n",
      "|           60|      28800|               5760.0|\n",
      "|          110|      20308|              10154.0|\n",
      "|           30|      24900|               4150.0|\n",
      "+-------------+-----------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round\n",
    "\n",
    "df.groupBy('department_id').agg(sum('salary'),round(avg('salary'),2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|department_id|sum(salary)|\n",
      "+-------------+-----------+\n",
      "|           20|      19000|\n",
      "|           40|       6500|\n",
      "|          100|      51608|\n",
      "|           10|       4400|\n",
      "|           50|      85600|\n",
      "|           70|      10000|\n",
      "|           90|      58000|\n",
      "|           60|      28800|\n",
      "|          110|      20308|\n",
      "|           30|      24900|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg,sum\n",
    "df.groupBy('department_id').agg({'salary':'sum'}).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation with constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+---------+--------+------------+------------+-------+------+--------------+----------+-------------+\n",
      "|employee_id|first_name|last_name|   email|PHONE_NUMBER|joining_date| job_id|salary|commission_pct|manager_id|department_id|\n",
      "+-----------+----------+---------+--------+------------+------------+-------+------+--------------+----------+-------------+\n",
      "|        200|  Jennifer|   Whalen| JWHALEN|515.123.4444|   17-SEP-03|AD_ASST|  4400|          NULL|       101|           10|\n",
      "|        201|   Michael|Hartstein|MHARTSTE|515.123.5555|   17-FEB-04| MK_MAN| 13000|          NULL|       100|           20|\n",
      "|        202|       Pat|      Fay|    PFAY|603.123.6666|   17-AUG-05| MK_REP|  6000|          NULL|       201|           20|\n",
      "+-----------+----------+---------+--------+------------+------------+-------+------+--------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.filter(df['department_id'].isin([10,20])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way of filtering\n",
    "from pyspark.sql.functions import col\n",
    "filterred_df = df.filter(col(\"department_id\").isin([10,20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+\n",
      "|department_id|sum(salary)|avg(salary)|\n",
      "+-------------+-----------+-----------+\n",
      "|           20|      19000|     9500.0|\n",
      "|           10|       4400|     4400.0|\n",
      "+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterred_df.groupBy(\"department_id\").agg(sum('salary'), avg('salary')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+-----------+\n",
      "|department_id|sum(salary)|avg(salary)|\n",
      "+-------------+-----------+-----------+\n",
      "|           10|       4400|     4400.0|\n",
      "|           20|      19000|     9500.0|\n",
      "+-------------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filterred_df.groupBy(\"department_id\").agg(sum('salary'), avg('salary')).orderBy('department_id', ascending=True).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregation using window function\n",
    "DEpartment wise total salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+--------+------------+------------+--------+------+--------------+----------+-------------+------------+---------+\n",
      "|employee_id|first_name|  last_name|   email|PHONE_NUMBER|joining_date|  job_id|salary|commission_pct|manager_id|department_id|total_salary|avgsalary|\n",
      "+-----------+----------+-----------+--------+------------+------------+--------+------+--------------+----------+-------------+------------+---------+\n",
      "|        200|  Jennifer|     Whalen| JWHALEN|515.123.4444|   17-SEP-03| AD_ASST|  4400|          NULL|       101|           10|        4400|   4400.0|\n",
      "|        201|   Michael|  Hartstein|MHARTSTE|515.123.5555|   17-FEB-04|  MK_MAN| 13000|          NULL|       100|           20|       19000|   9500.0|\n",
      "|        202|       Pat|        Fay|    PFAY|603.123.6666|   17-AUG-05|  MK_REP|  6000|          NULL|       201|           20|       19000|   9500.0|\n",
      "|        114|       Den|   Raphaely|DRAPHEAL|515.127.4561|   07-DEC-02|  PU_MAN| 11000|          NULL|       100|           30|       24900|   4150.0|\n",
      "|        115| Alexander|       Khoo|   AKHOO|515.127.4562|   18-MAY-03|PU_CLERK|  3100|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        116|    Shelli|      Baida|  SBAIDA|515.127.4563|   24-DEC-05|PU_CLERK|  2900|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        117|     Sigal|     Tobias| STOBIAS|515.127.4564|   24-JUL-05|PU_CLERK|  2800|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        118|       Guy|     Himuro| GHIMURO|515.127.4565|   15-NOV-06|PU_CLERK|  2600|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        119|     Karen| Colmenares|KCOLMENA|515.127.4566|   10-AUG-07|PU_CLERK|  2500|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        203|     Susan|     Mavris| SMAVRIS|515.123.7777|   07-JUN-02|  HR_REP|  6500|          NULL|       101|           40|        6500|   6500.0|\n",
      "|        198|    Donald|   OConnell|DOCONNEL|650.507.9833|   21-JUN-07|SH_CLERK|  2600|          NULL|       124|           50|       85600|  3721.74|\n",
      "|        199|   Douglas|      Grant|  DGRANT|650.507.9844|   13-JAN-08|SH_CLERK|  2600|          NULL|       124|           50|       85600|  3721.74|\n",
      "|        120|   Matthew|      Weiss|  MWEISS|650.123.1234|   18-JUL-04|  ST_MAN|  8000|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        121|      Adam|      Fripp|  AFRIPP|650.123.2234|   10-APR-05|  ST_MAN|  8200|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        122|     Payam|   Kaufling|PKAUFLIN|650.123.3234|   01-MAY-03|  ST_MAN|  7900|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        123|    Shanta|    Vollman|SVOLLMAN|650.123.4234|   10-OCT-05|  ST_MAN|  6500|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        124|     Kevin|    Mourgos|KMOURGOS|650.123.5234|   16-NOV-07|  ST_MAN|  5800|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        125|     Julia|      Nayer|  JNAYER|650.124.1214|   16-JUL-05|ST_CLERK|  3200|          NULL|       120|           50|       85600|  3721.74|\n",
      "|        126|     Irene|Mikkilineni|IMIKKILI|650.124.1224|   28-SEP-06|ST_CLERK|  2700|          NULL|       120|           50|       85600|  3721.74|\n",
      "|        127|     James|     Landry| JLANDRY|650.124.1334|   14-JAN-07|ST_CLERK|  2400|          NULL|       120|           50|       85600|  3721.74|\n",
      "+-----------+----------+-----------+--------+------------+------------+--------+------+--------------+----------+-------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window_spec = Window.partitionBy('department_id')\n",
    "\n",
    "df_result = df.withColumn('total_salary', sum('salary').over(window_spec))\\\n",
    "    .withColumn('avgsalary', round(avg('salary').over(window_spec),2))\n",
    "\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+--------+------------+------------+--------+------+--------------+----------+-------------+------------+---------+\n",
      "|employee_id|first_name|  last_name|   email|PHONE_NUMBER|joining_date|  job_id|salary|commission_pct|manager_id|department_id|total_salary|avgsalary|\n",
      "+-----------+----------+-----------+--------+------------+------------+--------+------+--------------+----------+-------------+------------+---------+\n",
      "|        200|  Jennifer|     Whalen| JWHALEN|515.123.4444|   17-SEP-03| AD_ASST|  4400|          NULL|       101|           10|        4400|   4400.0|\n",
      "|        201|   Michael|  Hartstein|MHARTSTE|515.123.5555|   17-FEB-04|  MK_MAN| 13000|          NULL|       100|           20|       19000|   9500.0|\n",
      "|        202|       Pat|        Fay|    PFAY|603.123.6666|   17-AUG-05|  MK_REP|  6000|          NULL|       201|           20|       19000|   9500.0|\n",
      "|        114|       Den|   Raphaely|DRAPHEAL|515.127.4561|   07-DEC-02|  PU_MAN| 11000|          NULL|       100|           30|       24900|   4150.0|\n",
      "|        115| Alexander|       Khoo|   AKHOO|515.127.4562|   18-MAY-03|PU_CLERK|  3100|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        116|    Shelli|      Baida|  SBAIDA|515.127.4563|   24-DEC-05|PU_CLERK|  2900|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        117|     Sigal|     Tobias| STOBIAS|515.127.4564|   24-JUL-05|PU_CLERK|  2800|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        118|       Guy|     Himuro| GHIMURO|515.127.4565|   15-NOV-06|PU_CLERK|  2600|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        119|     Karen| Colmenares|KCOLMENA|515.127.4566|   10-AUG-07|PU_CLERK|  2500|          NULL|       114|           30|       24900|   4150.0|\n",
      "|        203|     Susan|     Mavris| SMAVRIS|515.123.7777|   07-JUN-02|  HR_REP|  6500|          NULL|       101|           40|        6500|   6500.0|\n",
      "|        198|    Donald|   OConnell|DOCONNEL|650.507.9833|   21-JUN-07|SH_CLERK|  2600|          NULL|       124|           50|       85600|  3721.74|\n",
      "|        199|   Douglas|      Grant|  DGRANT|650.507.9844|   13-JAN-08|SH_CLERK|  2600|          NULL|       124|           50|       85600|  3721.74|\n",
      "|        120|   Matthew|      Weiss|  MWEISS|650.123.1234|   18-JUL-04|  ST_MAN|  8000|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        121|      Adam|      Fripp|  AFRIPP|650.123.2234|   10-APR-05|  ST_MAN|  8200|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        122|     Payam|   Kaufling|PKAUFLIN|650.123.3234|   01-MAY-03|  ST_MAN|  7900|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        123|    Shanta|    Vollman|SVOLLMAN|650.123.4234|   10-OCT-05|  ST_MAN|  6500|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        124|     Kevin|    Mourgos|KMOURGOS|650.123.5234|   16-NOV-07|  ST_MAN|  5800|          NULL|       100|           50|       85600|  3721.74|\n",
      "|        125|     Julia|      Nayer|  JNAYER|650.124.1214|   16-JUL-05|ST_CLERK|  3200|          NULL|       120|           50|       85600|  3721.74|\n",
      "|        126|     Irene|Mikkilineni|IMIKKILI|650.124.1224|   28-SEP-06|ST_CLERK|  2700|          NULL|       120|           50|       85600|  3721.74|\n",
      "|        127|     James|     Landry| JLANDRY|650.124.1334|   14-JAN-07|ST_CLERK|  2400|          NULL|       120|           50|       85600|  3721.74|\n",
      "+-----------+----------+-----------+--------+------------+------------+--------+------+--------------+----------+-------------+------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.orderBy(\"department_id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+------------+----------+\n",
      "|employee_id| FIRST_NAME|LAST_NAME|   EMAIL|PHONE_NUMBER|HIRE_DATE|    JOB_ID|SALARY|COMMISSION_PCT|MANAGER_ID|DEPARTMENT_ID|total_salary|avg_salary|\n",
      "+-----------+-----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+------------+----------+\n",
      "|        205|    Shelley|  Higgins|SHIGGINS|515.123.8080|07-JUN-02|    AC_MGR| 12008|          NULL|       101|          110|       20308|   10154.0|\n",
      "|        206|    William|    Gietz|  WGIETZ|515.123.8181|07-JUN-02|AC_ACCOUNT|  8300|          NULL|       205|          110|       20308|   10154.0|\n",
      "|        108|      Nancy|Greenberg|NGREENBE|515.124.4569|17-AUG-02|    FI_MGR| 12008|          NULL|       101|          100|       51608|   8601.33|\n",
      "|        109|     Daniel|   Faviet| DFAVIET|515.124.4169|16-AUG-02|FI_ACCOUNT|  9000|          NULL|       108|          100|       51608|   8601.33|\n",
      "|        110|       John|     Chen|   JCHEN|515.124.4269|28-SEP-05|FI_ACCOUNT|  8200|          NULL|       108|          100|       51608|   8601.33|\n",
      "|        111|     Ismael|  Sciarra|ISCIARRA|515.124.4369|30-SEP-05|FI_ACCOUNT|  7700|          NULL|       108|          100|       51608|   8601.33|\n",
      "|        112|Jose Manuel|    Urman| JMURMAN|515.124.4469|07-MAR-06|FI_ACCOUNT|  7800|          NULL|       108|          100|       51608|   8601.33|\n",
      "|        113|       Luis|     Popp|   LPOPP|515.124.4567|07-DEC-07|FI_ACCOUNT|  6900|          NULL|       108|          100|       51608|   8601.33|\n",
      "|        102|        Lex|  De Haan| LDEHAAN|515.123.4569|13-JAN-01|     AD_VP| 17000|          NULL|       100|           90|       58000|  19333.33|\n",
      "|        100|     Steven|     King|   SKING|515.123.4567|17-JUN-03|   AD_PRES| 24000|          NULL|      NULL|           90|       58000|  19333.33|\n",
      "|        101|      Neena|  Kochhar|NKOCHHAR|515.123.4568|21-SEP-05|     AD_VP| 17000|          NULL|       100|           90|       58000|  19333.33|\n",
      "|        204|    Hermann|     Baer|   HBAER|515.123.8888|07-JUN-02|    PR_REP| 10000|          NULL|       101|           70|       10000|   10000.0|\n",
      "|        103|  Alexander|   Hunold| AHUNOLD|590.423.4567|03-JAN-06|   IT_PROG|  9000|          NULL|       102|           60|       28800|    5760.0|\n",
      "|        104|      Bruce|    Ernst|  BERNST|590.423.4568|21-MAY-07|   IT_PROG|  6000|          NULL|       103|           60|       28800|    5760.0|\n",
      "|        105|      David|   Austin| DAUSTIN|590.423.4569|25-JUN-05|   IT_PROG|  4800|          NULL|       103|           60|       28800|    5760.0|\n",
      "|        106|      Valli|Pataballa|VPATABAL|590.423.4560|05-FEB-06|   IT_PROG|  4800|          NULL|       103|           60|       28800|    5760.0|\n",
      "|        107|      Diana|  Lorentz|DLORENTZ|590.423.5567|07-FEB-07|   IT_PROG|  4200|          NULL|       103|           60|       28800|    5760.0|\n",
      "|        133|      Jason|   Mallin| JMALLIN|650.127.1934|14-JUN-04|  ST_CLERK|  3300|          NULL|       122|           50|       85600|   3721.74|\n",
      "|        134|    Michael|   Rogers| MROGERS|650.127.1834|26-AUG-06|  ST_CLERK|  2900|          NULL|       122|           50|       85600|   3721.74|\n",
      "|        129|      Laura|   Bissot| LBISSOT|650.124.5234|20-AUG-05|  ST_CLERK|  3300|          NULL|       121|           50|       85600|   3721.74|\n",
      "+-----------+-----------+---------+--------+------------+---------+----------+------+--------------+----------+-------------+------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combined code\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import round\n",
    "\n",
    "win_spec = Window.partitionBy(\"department_id\")\n",
    "\n",
    "\n",
    "df.withColumn('total_salary',sum('salary').over(win_spec))\\\n",
    "    .withColumn('avg_salary',round(avg('salary').over(win_spec),2))\\\n",
    "    .orderBy('department_id',ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "catalyst optimization is the way of optimizing the large transformation so the it could be cost and time efficient by following these steps\n",
    "1. filter the data\n",
    "2. prune the data apply the broadcast joins if one data set is smaller\n",
    "3. logical execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.parquet(\"data1.parquet\")\n",
    "df2 = spark.read.parquet(\"data2.parquet\")\n",
    "\n",
    "result = df1.filter(df1[\"age\"] > 30) \\\n",
    "            .join(df2, df1[\"id\"] == df2[\"id\"]) \\\n",
    "            .groupBy(df1[\"city\"]) \\\n",
    "            .agg({\"salary\": \"avg\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take input from the stdin\n",
    "x=input();\n",
    "y=input();\n",
    "\n",
    "# Add two numbers\n",
    "z=int(x)+int(y);\n",
    "\n",
    "# Display the sum in the console.\n",
    "print ('Sum of x+y =',z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview scenario based\n",
    "\n",
    "Increment to the hr department only other will have same salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 10:30:41 WARN Utils: Your hostname, manu-pc resolves to a loopback address: 127.0.1.1; using 192.168.80.41 instead (on interface wlp58s0)\n",
      "24/09/06 10:30:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/06 10:30:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|name|hiked_salary|\n",
      "+----+------------+\n",
      "|John|      3600.0|\n",
      "|Jane|      4000.0|\n",
      "| Sam|      4200.0|\n",
      "| Max|      4500.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/06 10:31:00 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Salary_Hike\").getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "data = [(\"John\", \"HR\", 3000),\n",
    "        (\"Jane\", \"Finance\", 4000),\n",
    "        (\"Sam\", \"HR\", 3500),\n",
    "        (\"Max\", \"IT\", 4500)]\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\"name\", \"dept\", \"salary\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Apply CASE-like logic using when and otherwise\n",
    "df_with_hike = df.select(\n",
    "    col(\"name\"),\n",
    "    when(col(\"dept\") == \"HR\", col(\"salary\") + col(\"salary\") * 20 / 100)\n",
    "    .otherwise(col(\"salary\")).alias(\"hiked_salary\")\n",
    ")\n",
    "\n",
    "# Show result\n",
    "df_with_hike.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|name| hiked|\n",
      "+----+------+\n",
      "|John|3600.0|\n",
      "|Jane|4000.0|\n",
      "| Sam|4200.0|\n",
      "| Max|4500.0|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"name\", when(col(\"dept\")==\"HR\", col(\"salary\")+col(\"salary\")*20/100 ).otherwise(col('salary')).alias('hiked') ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find out the max rated employee from the employee table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder.appName(\"MaxRatedEmployeeByDepartment\").getOrCreate()\n",
    "\n",
    "# Sample employee data (assuming this is your DataFrame structure)\n",
    "data = [\n",
    "    (1, \"HR\", 4.5),\n",
    "    (2, \"Finance\", 3.8),\n",
    "    (3, \"Engineering\", 4.9),\n",
    "    (4, \"Engineering\", 4.2),\n",
    "    (5, \"HR\", 4.7),\n",
    "    (6, \"Finance\", 4.1),\n",
    "    (7, \"HR\", 4.3),\n",
    "    (8, \"Engineering\", 5.0),\n",
    "    (9, \"Finance\", 3.6),\n",
    "    (10, \"HR\", 4.0)\n",
    "]\n",
    "columns = [\"employee_id\", \"department\", \"rating\"]\n",
    "\n",
    "# Create a DataFrame with the sample data\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Sample 30 rows from the dataset (if the dataset is large, in this case we only have 10 rows)\n",
    "sample_df = df.sample(withReplacement=False, fraction=1.0).limit(30)\n",
    "\n",
    "# Group by department and find the employee with the maximum rating in each department\n",
    "max_rated_df = sample_df.groupBy(\"department\") \\\n",
    "    .agg({\"rating\": \"max\"}) \\\n",
    "    .withColumnRenamed(\"max(rating)\", \"max_rating\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================>                      (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| department|max(rating)|\n",
      "+-----------+-----------+\n",
      "|         HR|        4.7|\n",
      "|    Finance|        4.1|\n",
      "|Engineering|        5.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sample_df.groupBy(\"department\").max(\"rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+----------+\n",
      "|employee_id| department|rating|hgh_rating|\n",
      "+-----------+-----------+------+----------+\n",
      "|          3|Engineering|   4.9|       5.0|\n",
      "|          4|Engineering|   4.2|       5.0|\n",
      "|          8|Engineering|   5.0|       5.0|\n",
      "|          2|    Finance|   3.8|       4.1|\n",
      "|          6|    Finance|   4.1|       4.1|\n",
      "|          9|    Finance|   3.6|       4.1|\n",
      "|          1|         HR|   4.5|       4.7|\n",
      "|          5|         HR|   4.7|       4.7|\n",
      "|          7|         HR|   4.3|       4.7|\n",
      "|         10|         HR|   4.0|       4.7|\n",
      "+-----------+-----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using window function\n",
    "from pyspark.sql.functions import max\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "win_spec = Window.partitionBy(\"department\")\n",
    "\n",
    "sample_df.withColumn('hgh_rating', max('rating').over(win_spec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+\n",
      "|employee_id| department|rating|\n",
      "+-----------+-----------+------+\n",
      "|          1|         HR|   4.5|\n",
      "|          2|    Finance|   3.8|\n",
      "|          3|Engineering|   4.9|\n",
      "|          4|Engineering|   4.2|\n",
      "|          5|         HR|   4.7|\n",
      "|          6|    Finance|   4.1|\n",
      "|          7|         HR|   4.3|\n",
      "|          8|Engineering|   5.0|\n",
      "|          9|    Finance|   3.6|\n",
      "|         10|         HR|   4.0|\n",
      "+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "| department|max_rating|\n",
      "+-----------+----------+\n",
      "|         HR|       4.7|\n",
      "|    Finance|       4.1|\n",
      "|Engineering|       5.0|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_rated_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+\n",
      "|employee_id| department|rating|\n",
      "+-----------+-----------+------+\n",
      "|          5|         HR|   4.7|\n",
      "|          6|    Finance|   4.1|\n",
      "|          8|Engineering|   5.0|\n",
      "+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join with the original DataFrame to get employee details for the max-rated employees\n",
    "result_df = max_rated_df.alias('m') \\\n",
    "    .join(sample_df.alias('s'), \n",
    "          (col('s.rating') == col('m.max_rating')) & (col('s.department') == col('m.department')), \n",
    "          how='inner') \\\n",
    "    .select(col('s.employee_id'), col('s.department'), col('s.rating'))\n",
    "\n",
    "# Show the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------+\n",
      "|employee_id| department|rating|\n",
      "+-----------+-----------+------+\n",
      "|          5|         HR|   4.7|\n",
      "|          6|    Finance|   4.1|\n",
      "|          8|Engineering|   5.0|\n",
      "+-----------+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_rated_df.alias('m').join(sample_df.alias('s'), (col('s.rating') == col(\"m.max_rating\"))\\\n",
    "                                                    & (col('s.department') == col(\"m.department\")),how='inner')\\\n",
    "                                                    .select(col('s.employee_id'), col('s.department'), col('s.rating')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
