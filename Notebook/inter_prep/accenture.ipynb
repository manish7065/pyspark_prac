{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/16 20:29:36 WARN Utils: Your hostname, manu-pc resolves to a loopback address: 127.0.1.1; using 192.168.157.41 instead (on interface wlp58s0)\n",
      "24/08/16 20:29:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/16 20:29:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|  name|marks|standard|\n",
      "+------+-----+--------+\n",
      "| anshu|   95|       4|\n",
      "|aparna|   96|       4|\n",
      "|   roy|   93|       5|\n",
      "|  abhi|   95|       4|\n",
      "|   sam|   96|       5|\n",
      "+------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:=====================>                                    (3 + 5) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+--------+\n",
      "|high_scorer|highest_mark|standard|\n",
      "+-----------+------------+--------+\n",
      "|      anshu|          95|       4|\n",
      "|        roy|          93|       5|\n",
      "|       abhi|          95|       4|\n",
      "+-----------+------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [(\"anshu\", 95, 4),\n",
    "        ('aparna', 96, 4),\n",
    "        ('roy', 93, 5),\n",
    "        ('abhi', 95, 4),\n",
    "        ('sam', 96, 5)\n",
    "\n",
    "]\n",
    "column = ['name', 'marks', 'standard']\n",
    "\n",
    "df = spark.createDataFrame(data,column)\n",
    "df.show()\n",
    "querry  = \"\"\"\n",
    "with high_table as(\n",
    "    select name, Rank() over ( partition by standard order by marks) as rnk\n",
    "    from table\n",
    ")\n",
    "select name as high_scorer, marks as highest_mark, standard from table\n",
    "where name in (\n",
    "    select name from high_table\n",
    "    where rnk = 1\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "df.createOrReplaceTempView(\"table\")\n",
    "spark.sql(querry).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+------+\n",
      "|  name|marks|standard|remark|\n",
      "+------+-----+--------+------+\n",
      "| anshu|   95|       4|  good|\n",
      "|aparna|   96|       4| great|\n",
      "|   roy|   93|       5|  good|\n",
      "|  abhi|   96|       4| great|\n",
      "|   sam|   96|       5| great|\n",
      "+------+-----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "data = [(\"anshu\", 95, 4),\n",
    "        ('aparna', 96, 4),\n",
    "        ('roy', 93, 5),\n",
    "        ('abhi', 96, 4),\n",
    "        ('sam', 96, 5)\n",
    "]\n",
    "columns = ['name', 'marks', 'standard']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "# df.show()\n",
    "df.withColumn(\"remark\", expr(\"case when marks > 95 then 'great' else 'good' end\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+------+\n",
      "|  name|marks|standard|remark|\n",
      "+------+-----+--------+------+\n",
      "| anshu|   95|       4|  good|\n",
      "|aparna|   96|       4| great|\n",
      "|   roy|   93|       5|  good|\n",
      "|  abhi|   96|       4| great|\n",
      "|   sam|   96|       5| great|\n",
      "+------+-----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "data = [(\"anshu\", 95, 4),\n",
    "        ('aparna', 96, 4),\n",
    "        ('roy', 93, 5),\n",
    "        ('abhi', 96, 4),\n",
    "        ('sam', 96, 5)\n",
    "]\n",
    "columns = ['name', 'marks', 'standard']\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# categorization using sql querry\n",
    "df = df.withColumn(\"remark\", expr(\"CASE WHEN marks > 95 THEN 'great' ELSE 'good' END\"))\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+-------+\n",
      "|            name|         name_length| f_name|\n",
      "+----------------+--------------------+-------+\n",
      "|    manish kumar|     [manish, kumar]| manish|\n",
      "|nil nitin mukesh|[nil, nitin, mukesh]|    nil|\n",
      "|         saurabh|           [saurabh]|saurabh|\n",
      "+----------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Name Length count\n",
    "from pyspark.sql.functions import col,split\n",
    "\n",
    "data = [\n",
    "    (\"manish kumar\",),\n",
    "    (\"nil nitin mukesh\",),\n",
    "    ('saurabh',)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data,['name'])\n",
    "\n",
    "df = df.withColumn(\"name_length\", split(col(\"name\"), \" \"))\n",
    "df = df.withColumn(\"f_name\", split(col(\"name\"), \" \")[0])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- name_length: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------+\n",
      "|            name|name_length|\n",
      "+----------------+-----------+\n",
      "|    manish kumar|          2|\n",
      "|nil nitin mukesh|          3|\n",
      "|         saurabh|          1|\n",
      "+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "df.withColumn(\"name_length\", size(split(col(\"name\"), \" \"))).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
