{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crete a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| ali| 13|\n",
      "| ram| 22|\n",
      "|sonu| 25|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()\n",
    "\n",
    "data = [('ali',13),('ram',22),('sonu',25)]\n",
    "columns = ['name','age']\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|AgePlusOne|\n",
      "+----+----------+\n",
      "| ali|        14|\n",
      "| ram|        23|\n",
      "|sonu|        26|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"Name\", \"Age + 1 as AgePlusOne\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tempview using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| ram| 22|\n",
      "|sonu| 25|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"example\")\n",
    "spark.sql(\"select * from example where age>15\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------+--------+---------+----+-----------+----------+\n",
      "|Property_ID|      Location| Price|Bedrooms|Bathrooms|Size|Price_SQ_FT|    Status|\n",
      "+-----------+--------------+------+--------+---------+----+-----------+----------+\n",
      "|    1461262| Arroyo Grande|795000|       3|        3|2371|      365.3|Short Sale|\n",
      "|    1478004|   Paulo Pablo|399000|       4|        3|2818|     163.59|Short Sale|\n",
      "|    1486551|   Paulo Pablo|545000|       4|        3|3032|     179.75|Short Sale|\n",
      "|    1492832|     Santa Bay|909000|       4|        4|3540|     286.78|Short Sale|\n",
      "|    1499102|Thomas Country|109900|       3|        1|1249|      98.99|Short Sale|\n",
      "|    1489132|Thomas Country|109000|       2|        1|1129|      93.99|Short Sale|\n",
      "|    1467262|    Fort Worth|987000|       4|        3|2771|      465.3|Short Sale|\n",
      "|    1478114|   Paulo Pablo|409000|       4|        3|2918|     223.19|Short Sale|\n",
      "|    1402551|     Nashville|545000|       4|        3|2932|     169.75|Short Sale|\n",
      "|    1405832|      San Jose|980000|       4|        4|3340|     290.98|Short Sale|\n",
      "|    1493302|    Fort Worth|119900|       3|        2|2249|     198.99|Short Sale|\n",
      "|    1412332|Thomas Country|129000|       3|        2|1329|      73.99|Short Sale|\n",
      "|    1469062| Arroyo Grande|798000|       3|        4|2321|      235.9|Short Sale|\n",
      "|    1498004|     Nashville|789000|       4|        3|2419|     263.59|Short Sale|\n",
      "|    1586751|     Nashville|896000|       4|        3|3132|     199.75|Short Sale|\n",
      "|    1433232|      Glendale|987000|       4|        4|3340|     216.78|Short Sale|\n",
      "|    1495502|    Fort Worth|219900|       3|        2| 987|     200.99|Short Sale|\n",
      "|    1489100|      San Jose|107200|       1|        1| 789|      78.99|Short Sale|\n",
      "+-----------+--------------+------+--------+---------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/input_real_estate.txt\"\n",
    "df = spark.read.option('delimiter','|').csv(file_path,header=True,inferSchema=True)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+---+---+\n",
      "|   _c0|  _c1|     _c2|_c3|_c4|\n",
      "+------+-----+--------+---+---+\n",
      "|Robert| null|Williams|  3|  M|\n",
      "| Rajiv| Mary|   Kumar|  5|  F|\n",
      "|Oliver|Queen|    null|  2|  M|\n",
      "| Berry| null|   Allen|  1|  M|\n",
      "|  Tony| null|   Stark|  4|  F|\n",
      "+------+-----+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../record.csv\"\n",
    "df = spark.read.csv(file_path)\n",
    "df.show()\n",
    "coalesce_df = df.coalesce(1)\n",
    "coalesce_df.write.csv(\"../data/record.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diffrence in groupby and reduceby "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"GroupByExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\",\"HR\",3000),('Bob', \"Engineering\", 4000), ('carol','HR',3500),('David','Engineering',4500)]\n",
    "columns = ['Name', 'Department', 'Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+\n",
      "| Name| Department|Salary|\n",
      "+-----+-----------+------+\n",
      "|Alice|         HR|  3000|\n",
      "|  Bob|Engineering|  4000|\n",
      "|carol|         HR|  3500|\n",
      "|David|Engineering|  4500|\n",
      "+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|avg(Salary)|\n",
      "+-----------+-----------+\n",
      "|         HR|     3250.0|\n",
      "|Engineering|     4250.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group by Department and compute average salary\n",
    "grouped_df = df.groupBy(\"Department\").avg(\"Salary\")\n",
    "grouped_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|max(Salary)|\n",
      "+-----------+-----------+\n",
      "|         HR|       3500|\n",
      "|Engineering|       4500|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_df = df.groupBy(\"Department\").max(\"Salary\")\n",
    "max_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "| Department|max_s|\n",
      "+-----------+-----+\n",
      "|         HR| 3500|\n",
      "|Engineering| 4500|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "max_df = df.groupBy(\"Department\").agg(max(\"Salary\").alias(\"max_s\"))\n",
    "max_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
