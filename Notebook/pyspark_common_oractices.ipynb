{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crete a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| ali| 13|\n",
      "| ram| 22|\n",
      "|sonu| 25|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()\n",
    "\n",
    "data = [('ali',13),('ram',22),('sonu',25)]\n",
    "columns = ['name','age']\n",
    "\n",
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|AgePlusOne|\n",
      "+----+----------+\n",
      "| ali|        14|\n",
      "| ram|        23|\n",
      "|sonu|        26|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"Name\", \"Age + 1 as AgePlusOne\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.selectExpr(\"Name\",\"Age\", \"Age + 1 as AgePlusOne\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tempview using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "| ram| 22|\n",
      "|sonu| 25|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"example\")\n",
    "spark.sql(\"select * from example where age>15\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+------+--------+---------+----+-----------+----------+\n",
      "|Property_ID|      Location| Price|Bedrooms|Bathrooms|Size|Price_SQ_FT|    Status|\n",
      "+-----------+--------------+------+--------+---------+----+-----------+----------+\n",
      "|    1461262| Arroyo Grande|795000|       3|        3|2371|      365.3|Short Sale|\n",
      "|    1478004|   Paulo Pablo|399000|       4|        3|2818|     163.59|Short Sale|\n",
      "|    1486551|   Paulo Pablo|545000|       4|        3|3032|     179.75|Short Sale|\n",
      "|    1492832|     Santa Bay|909000|       4|        4|3540|     286.78|Short Sale|\n",
      "|    1499102|Thomas Country|109900|       3|        1|1249|      98.99|Short Sale|\n",
      "|    1489132|Thomas Country|109000|       2|        1|1129|      93.99|Short Sale|\n",
      "|    1467262|    Fort Worth|987000|       4|        3|2771|      465.3|Short Sale|\n",
      "|    1478114|   Paulo Pablo|409000|       4|        3|2918|     223.19|Short Sale|\n",
      "|    1402551|     Nashville|545000|       4|        3|2932|     169.75|Short Sale|\n",
      "|    1405832|      San Jose|980000|       4|        4|3340|     290.98|Short Sale|\n",
      "|    1493302|    Fort Worth|119900|       3|        2|2249|     198.99|Short Sale|\n",
      "|    1412332|Thomas Country|129000|       3|        2|1329|      73.99|Short Sale|\n",
      "|    1469062| Arroyo Grande|798000|       3|        4|2321|      235.9|Short Sale|\n",
      "|    1498004|     Nashville|789000|       4|        3|2419|     263.59|Short Sale|\n",
      "|    1586751|     Nashville|896000|       4|        3|3132|     199.75|Short Sale|\n",
      "|    1433232|      Glendale|987000|       4|        4|3340|     216.78|Short Sale|\n",
      "|    1495502|    Fort Worth|219900|       3|        2| 987|     200.99|Short Sale|\n",
      "|    1489100|      San Jose|107200|       1|        1| 789|      78.99|Short Sale|\n",
      "+-----------+--------------+------+--------+---------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Demo\").getOrCreate()\n",
    "\n",
    "file_path = \"../data/input_real_estate.txt\"\n",
    "df = spark.read.option('delimiter','|').csv(file_path,header=True,inferSchema=True)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+---+---+\n",
      "|   _c0|  _c1|     _c2|_c3|_c4|\n",
      "+------+-----+--------+---+---+\n",
      "|Robert| null|Williams|  3|  M|\n",
      "| Rajiv| Mary|   Kumar|  5|  F|\n",
      "|Oliver|Queen|    null|  2|  M|\n",
      "| Berry| null|   Allen|  1|  M|\n",
      "|  Tony| null|   Stark|  4|  F|\n",
      "+------+-----+--------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"../record.csv\"\n",
    "df = spark.read.csv(file_path)\n",
    "df.show()\n",
    "coalesce_df = df.coalesce(1)\n",
    "coalesce_df.write.csv(\"../data/record.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## diffrence in groupby and reduceby "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"GroupByExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"Alice\",\"HR\",3000),('Bob', \"Engineering\", 4000), ('carol','HR',3500),('David','Engineering',4500)]\n",
    "columns = ['Name', 'Department', 'Salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+------+\n",
      "| Name| Department|Salary|\n",
      "+-----+-----------+------+\n",
      "|Alice|         HR|  3000|\n",
      "|  Bob|Engineering|  4000|\n",
      "|carol|         HR|  3500|\n",
      "|David|Engineering|  4500|\n",
      "+-----+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(data,columns)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|avg(Salary)|\n",
      "+-----------+-----------+\n",
      "|         HR|     3250.0|\n",
      "|Engineering|     4250.0|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Group by Department and compute average salary\n",
    "grouped_df = df.groupBy(\"Department\").avg(\"Salary\")\n",
    "grouped_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "| Department|max(Salary)|\n",
      "+-----------+-----------+\n",
      "|         HR|       3500|\n",
      "|Engineering|       4500|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_df = df.groupBy(\"Department\").max(\"Salary\")\n",
    "max_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "| Department|max_s|\n",
      "+-----------+-----+\n",
      "|         HR| 3500|\n",
      "|Engineering| 4500|\n",
      "+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "max_df = df.groupBy(\"Department\").agg(max(\"Salary\").alias(\"max_s\"))\n",
    "max_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing the Schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Property_ID: integer (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Price: integer (nullable = true)\n",
      " |-- Bedrooms: integer (nullable = true)\n",
      " |-- Bathrooms: integer (nullable = true)\n",
      " |-- Size: integer (nullable = true)\n",
      " |-- Price_SQ_FT: double (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| Price|\n",
      "+------+\n",
      "|795000|\n",
      "|399000|\n",
      "|545000|\n",
      "|909000|\n",
      "|109900|\n",
      "|109000|\n",
      "|987000|\n",
      "|409000|\n",
      "|545000|\n",
      "|980000|\n",
      "|119900|\n",
      "|129000|\n",
      "|798000|\n",
      "|789000|\n",
      "|896000|\n",
      "|987000|\n",
      "|219900|\n",
      "|107200|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------+--------+---------+----+-----------+----------+\n",
      "|Property_ID|     Location| Price|Bedrooms|Bathrooms|Size|Price_SQ_FT|    Status|\n",
      "+-----------+-------------+------+--------+---------+----+-----------+----------+\n",
      "|    1461262|Arroyo Grande|795000|       3|        3|2371|      365.3|Short Sale|\n",
      "|    1486551|  Paulo Pablo|545000|       4|        3|3032|     179.75|Short Sale|\n",
      "|    1492832|    Santa Bay|909000|       4|        4|3540|     286.78|Short Sale|\n",
      "|    1467262|   Fort Worth|987000|       4|        3|2771|      465.3|Short Sale|\n",
      "|    1402551|    Nashville|545000|       4|        3|2932|     169.75|Short Sale|\n",
      "|    1405832|     San Jose|980000|       4|        4|3340|     290.98|Short Sale|\n",
      "|    1469062|Arroyo Grande|798000|       3|        4|2321|      235.9|Short Sale|\n",
      "|    1498004|    Nashville|789000|       4|        3|2419|     263.59|Short Sale|\n",
      "|    1586751|    Nashville|896000|       4|        3|3132|     199.75|Short Sale|\n",
      "|    1433232|     Glendale|987000|       4|        4|3340|     216.78|Short Sale|\n",
      "+-----------+-------------+------+--------+---------+----+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df['Price']>500000).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By Aggregation\n",
    "<u>count()</u>\n",
    "<u>countDistinct()</u>\n",
    "<u>sum()</u>\n",
    "<u>avg()</u>\n",
    "<u>min()</u>\n",
    "<u>max()</u>\n",
    "<u>last()</u>\n",
    "<u>variance()</u>\n",
    "<u>stddev()</u>\n",
    "<u>skewness()</u>\n",
    "<u>kurtosis()</u>\n",
    "<u>approx_count_distinct()</u>\n",
    "<u>collect_list()</u>\n",
    "<u>collect_set()</u>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|      Location|sum(price)|\n",
      "+--------------+----------+\n",
      "|     Nashville|   2230000|\n",
      "|   Paulo Pablo|   1353000|\n",
      "| Arroyo Grande|   1593000|\n",
      "|      Glendale|    987000|\n",
      "|     Santa Bay|    909000|\n",
      "|      San Jose|   1087200|\n",
      "|Thomas Country|    347900|\n",
      "|    Fort Worth|   1326800|\n",
      "+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Location\").agg({'price':\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|      Location|count(Bathrooms)|\n",
      "+--------------+----------------+\n",
      "|     Nashville|               3|\n",
      "|   Paulo Pablo|               3|\n",
      "| Arroyo Grande|               2|\n",
      "|      Glendale|               1|\n",
      "|     Santa Bay|               1|\n",
      "|      San Jose|               2|\n",
      "|Thomas Country|               3|\n",
      "|    Fort Worth|               3|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"Location\").agg({\"Bathrooms\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+\n",
      "|      Location|count(Bathrooms)|\n",
      "+--------------+----------------+\n",
      "|     Nashville|               3|\n",
      "|   Paulo Pablo|               3|\n",
      "| Arroyo Grande|               2|\n",
      "|      Glendale|               1|\n",
      "|     Santa Bay|               1|\n",
      "|      San Jose|               2|\n",
      "|Thomas Country|               3|\n",
      "|    Fort Worth|               3|\n",
      "+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.select(countDistinct(\"Bathrooms\").alias(\"dist_count\")).show()\n",
    "df.groupBy(\"Location\").agg({\"Bathrooms\":\"count\"}).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|last_bath|\n",
      "+---------+\n",
      "|        1|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import last\n",
    "\n",
    "last_df = df.select(last(\"Bathrooms\").alias(\"last_bath\"))\n",
    "last_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     var_samp(Price)|\n",
      "+--------------------+\n",
      "|1.230593021241830...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import variance\n",
    "df.select(variance('Price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|deviation_in_price|\n",
      "+------------------+\n",
      "| 350798.0931022616|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import stddev\n",
    "df.select(stddev('Price').alias(\"deviation_in_price\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     skewness(Price)|\n",
      "+--------------------+\n",
      "|-0.07496267236398088|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness\n",
    "df.select(skewness('Price')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------+\n",
      "|Location      |price_range             |\n",
      "+--------------+------------------------+\n",
      "|Nashville     |[545000, 789000, 896000]|\n",
      "|Paulo Pablo   |[399000, 545000, 409000]|\n",
      "|Arroyo Grande |[795000, 798000]        |\n",
      "|Glendale      |[987000]                |\n",
      "|Santa Bay     |[909000]                |\n",
      "|San Jose      |[980000, 107200]        |\n",
      "|Thomas Country|[109900, 109000, 129000]|\n",
      "|Fort Worth    |[987000, 119900, 219900]|\n",
      "+--------------+------------------------+\n",
      "\n",
      "+--------------+------------------------+\n",
      "|Location      |price_range             |\n",
      "+--------------+------------------------+\n",
      "|Nashville     |[545000, 896000, 789000]|\n",
      "|Paulo Pablo   |[545000, 399000, 409000]|\n",
      "|Arroyo Grande |[798000, 795000]        |\n",
      "|Glendale      |[987000]                |\n",
      "|Santa Bay     |[909000]                |\n",
      "|San Jose      |[980000, 107200]        |\n",
      "|Thomas Country|[109900, 109000, 129000]|\n",
      "|Fort Worth    |[987000, 119900, 219900]|\n",
      "+--------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_list,collect_set\n",
    "df.groupBy('Location').agg(collect_list('Price').alias(\"price_range\")).show(truncate=False)\n",
    "df.groupBy('Location').agg(collect_set('Price').alias(\"price_range\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| name|value_list|\n",
      "+-----+----------+\n",
      "|Alice|    [1, 3]|\n",
      "|  Bob|    [2, 4]|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_list\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder.appName(\"collect_list Example\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame with sample data\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Alice\", 3), (\"Bob\", 4)]\n",
    "df = spark.createDataFrame(data, [\"name\", \"value\"])\n",
    "\n",
    "# Using collect_list to aggregate values\n",
    "collect_list_df = df.groupBy(\"name\").agg(collect_list(\"value\").alias(\"value_list\"))\n",
    "collect_list_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
