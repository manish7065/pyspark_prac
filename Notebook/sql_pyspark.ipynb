{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Querries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/16 10:35:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"demo\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data in DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "file_path = \"../data/record.csv\"\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\",\"true\") \\\n",
    "    .option('inferSchema','true') \\\n",
    "    .csv(file_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "    StructField('f_name',StringType(),True),\n",
    "    StructField('m_name',StringType(),True),\n",
    "    StructField('l_name',StringType(),True),\n",
    "    StructField('age',IntegerType(),True),\n",
    "    StructField('sex',StringType(),True)\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).csv(file_apth, header=True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+---+---+\n",
      "|f_name|m_name|l_name|age|sex|\n",
      "+------+------+------+---+---+\n",
      "| Rajiv|  Mary| Kumar|  5|  F|\n",
      "|Oliver| Queen|  null|  2|  M|\n",
      "| Berry|  null| Allen|  1|  M|\n",
      "|  Tony|  null| Stark|  4|  F|\n",
      "+------+------+------+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/16 11:36:23 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Robert, , Williams, 3, M\n",
      " Schema: f_name, m_name, l_name, age, sex\n",
      "Expected: f_name but found: Robert\n",
      "CSV file: file:///config/workspace/data/record.csv/part-00000-eafc1665-bac4-444e-929d-9a6cdd630f62-c000.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sales data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "slaes_file_path = \"../data/sales_data.csv\"\n",
    "\n",
    "df = spark.read.csv(slaes_file_path,header= True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+-----+----------+\n",
      "|OrderID|ProductID|Quantity|Price| OrderDate|\n",
      "+-------+---------+--------+-----+----------+\n",
      "|      1|      101|       3|  100|2023-01-01|\n",
      "|      2|      102|       1|  200|2023-01-02|\n",
      "|      3|      101|       2|  100|2023-01-03|\n",
      "|      4|      103|       5|   50|2023-01-04|\n",
      "+-------+---------+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('sales_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|ProductId|total_sales|\n",
      "+---------+-----------+\n",
      "|      101|        500|\n",
      "|      103|        250|\n",
      "|      102|        200|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT ProductId, sum(Price*Quantity) as total_sales from sales_data\n",
    "GROUP BY ProductId\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|ProductId|total_sales|\n",
      "+---------+-----------+\n",
      "|      101|        500|\n",
      "|      103|        250|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# top 2 prduct by revenue\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ProductId, sum(Price*Quantity) as total_sales from sales_data\n",
    "GROUP BY ProductId ORDER BY total_sales DESC\n",
    "LIMIT 2\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# employee table manupulation using sql querry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_path = '../data/employee/employee.csv'\n",
    "proj_path = '../data/employee/project.csv'\n",
    "\n",
    "emp_df = spark.read.csv(emp_path, sep = \"\\t\" ,header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|employee_id name    experience_years|\n",
      "+------------------------------------+\n",
      "|                            1   raam|\n",
      "|                                   2|\n",
      "|                                   3|\n",
      "|                                   4|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
